{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"24c642d3-0b11-4920-a84b-4baefc90fa05","_uuid":"db9d0665-9697-41b2-99b9-50be8407af67","trusted":true},"source":["# Model Training Jupyter Notebook"]},{"cell_type":"markdown","metadata":{},"source":["### Importing packages and loading data"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b9f4030b-ebe7-4c4b-bde1-33c15c125d11","_uuid":"0057232c-4912-46c9-b260-df2da835b150","collapsed":false,"execution":{"iopub.execute_input":"2023-12-13T01:29:52.629085Z","iopub.status.busy":"2023-12-13T01:29:52.628630Z","iopub.status.idle":"2023-12-13T01:29:55.198913Z","shell.execute_reply":"2023-12-13T01:29:55.197662Z","shell.execute_reply.started":"2023-12-13T01:29:52.629047Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import plotly.express as px\n","import plotly.graph_objects as g\n","from scipy.stats import uniform, randint\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","from sklearn.metrics import mean_squared_error\n","\n","from sklearn.ensemble import HistGradientBoostingRegressor\n","\n","from sklearn.inspection import permutation_importance"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["### Importing data\n","# Reading data\n","df_train = pd.read_csv('data/train_logs.csv', \n","                 header=0)\n","df_test = pd.read_csv('data/test_logs.csv', \n","                 header=0)\n","df_train_scores = pd.read_csv('data/train_scores.csv')"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Engineering"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T01:29:55.201857Z","iopub.status.busy":"2023-12-13T01:29:55.201142Z","iopub.status.idle":"2023-12-13T01:29:55.212030Z","shell.execute_reply":"2023-12-13T01:29:55.210698Z","shell.execute_reply.started":"2023-12-13T01:29:55.201822Z"},"trusted":true},"outputs":[],"source":["def constructor(df):\n","    \"\"\"\n","    Group words and sentences in a DataFrame based on the 'activity' and 'down_events' column. This constructs a \n","\n","    Parameters:\n","    df (DataFrame): The input DataFrame containing the 'activity' column.\n","\n","    Returns:\n","    DataFrame: The modified DataFrame with additional columns 'word_start' and 'word_end' indicating the boundaries of words.\n","    \"\"\"\n","    \n","    # Initialize columns for word and sentence beginnings and endings\n","    df['word_start'] = 0\n","    df['word_end'] = 0\n","    df['sentence_start'] = 0\n","    df['sentence_end'] = 0\n","\n","    \n","    # Shifting the activity columns up and down one for subsequent calculations\n","    shifted_activity_prev = df['activity'].shift(1)\n","    shifted_activity_next = df['activity'].shift(-1)\n","    \n","    # Identification of word boundaries using the 'activity' column\n","    df['word_start'] = ((df['activity'] == 'Input') & (shifted_activity_prev != 'Input')).astype(int)\n","    df['word_end'] = ((df['activity'] == 'Input') & (shifted_activity_next != 'Input')).astype(int)\n","    \n","    # Identification of sentence boundaries\n","   # df['sentence_start']\n","\n","    # Handling edge cases: adressing first and last column of dataframe\n","    df.at[0, 'word_start'] = int(df.iloc[0]['activity'] == 'Input')\n","    df.at[df.index[-1], 'word_end'] = int(df.iloc[-1]['activity'] == 'Input')\n","    \n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### Transition the Feature Engineering to the 'down_event' and/or the 'up_event' column to construct a word."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def constructor_new(df):\n","\n","    # Initialize columns for word and sentence beginnings and endings\n","    df['word_start'] = 0\n","    df['word_end'] = 0\n","    df['sentence_start'] = 0\n","    df['sentence_end'] = 0\n","\n","    # Mask when down_event and up_event are not equal\n","    if df['down_event'] != df['up_event']:\n","        df['up_event'] = df['down_event']\n","        "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def getEssays(df):\n","    # Copy required columns\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n","    \n","    # Get rid of text inputs that make no change\n","    # Note: Shift was unpreditcable so ignored\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","\n","    # Get how much each Id there is\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","\n","    # Holds the final index of the previous Id\n","    lastIndex = 0\n","\n","    # Holds all the essays\n","    essaySeries = pd.Series()\n","\n","    # Fills essay series with essays\n","    for index, valCount in enumerate(valCountsArr):\n","\n","        # Indexes down_time at current Id\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","\n","        # Update the last index\n","        lastIndex += valCount\n","\n","        # Where the essay content will be stored\n","        essayText = \"\"\n","\n","        \n","        # Produces the essay\n","        for Input in currTextInput.values:\n","            \n","            # Input[0] = activity\n","            # Input[2] = cursor_position\n","            # Input[3] = text_change\n","            \n","            # If activity = Replace\n","            if Input[0] == 'Replace':\n","                # splits text_change at ' => '\n","                replaceTxt = Input[2].split(' => ')\n","                \n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","\n","                \n","            # If activity = Paste    \n","            if Input[0] == 'Paste':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Remove/Cut\n","            if Input[0] == 'Remove/Cut':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Move...\n","            if \"M\" in Input[0]:\n","                # Gets rid of the \"Move from to\" text\n","                croppedTxt = Input[0][10:]\n","                \n","                # Splits cropped text by ' To '\n","                splitTxt = croppedTxt.split(' To ')\n","                \n","                # Splits split text again by ', ' for each item\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                \n","                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n","                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","\n","                # Skip if someone manages to activiate this by moving to same place\n","                if moveData[0] != moveData[2]:\n","                    # Check if they move text forward in essay (they are different)\n","                    if moveData[0] < moveData[2]:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","                \n","                \n","            # If just input\n","            # DONT TOUCH\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","\n","            \n","        # Sets essay at index  \n","        essaySeries[index] = essayText\n","     \n","    \n","    # Sets essay series index to the ids\n","    essaySeries.index =  textInputDf['id'].unique()\n","    \n","    \n","    # Returns the essay series\n","    return essaySeries"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 3min 37s, sys: 37.4 s, total: 4min 15s\n","Wall time: 4min 15s\n"]}],"source":["%%time\n","essays = getEssays(df_train)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq qqqq qqqqqq qq qq qqqqq qq qqqq qqqqq qq qqqqqqqqq qqqqq qqqq qqqqq qqq qqqqqqqqq qqqqqqqqq qqqq.  qqqqqq qqq qqqqq qqq qqqqqqqqqqq qq qqq qqqqqqqqqq qqqqq, qqq qqqqq qqqqqq qq qq qqqq qqq qqqqqq qqqqqqq qq qqq qqqqqqqqqqq.  qqqqqqqq qq qqqqqqqqqq qqqq qqqq qqqqqqqqq qqq qqqqqqq qq qqqqqq qqqq qqq qqq qq qqqqqqqqq qq qq qqq qqqqq qqqqq qq qqq.\n","\n","qq qq qqqq qqqq qqq qqqqqqqqq qqq qqqqqqq qq qqq qqqqq qqqqq, qq qq qqqqqq qqq qqq qqqqqqqq qqqqq qq qqq qqqqqqqqqqq qq qqqqqqqqq.  qqqqqqqqq qq qqq qqqqqqqq qqqq qq qqqq qq qqqqqqq qqqqq qqqqq, qqq qqqqqq qqqqq qqqqq qqq qqq qq qqq qqqqqqq qqqqqqq qqqq.  qqqq qqqqq qqqqq qqqq qqqq'qq qqqqq qqqqqqqqq qqqqq qqqqqqq qqqqqqq qqqqqqqqqq, qqqq qq qqqqqqqqqq qqqqqqq qqq qqqqqqq; qqqqqqq, qqqqq qqqqqqqq qqqqqq qqqqqqq qqqqqqq qqq qqqqq qqq qqq qqq qqqqqqq.  qqqq qqqqqqqqq qqqq qqq qqqq qqqq qqqqq qqqqqqqqqq qqqq qqqqq qqqqq.  qqq qqqqqqqqqq qq qqqqqqqq q qqqqqq, qqqqqqqq qqqq qqqq qqqqqqqqqq, qqq. qq qqqqq qq qqqqq qqqqqqq qqqqqqqqq qq qq qqqq qqqqqqq. \n","\n","qqqq qqq qqqqqq qqqqqqqqqq qqqqqqqqq qqqqq, qqqqqq qqqqq qqq qqqqqq qqqq qq qqqqqq qqqqqqq qqqqq qqq qqqqqqqqq qqqq qqq qqqqq qq qqq.  qq qqqqq qqqqqqq qqq qqqqq qq qqqqqq qqq qqqqqq qqqqq qqq qqqqq qq qqqqqqqq qqqqqqqq qq qqqqqqqqq qqqq qq qqqq qq qqqqqq.  qqqqqqqq qq qqq qqqqqqqqqqq qq qqqqqqqq, qqq qqqqqqq qqqqqqqqq qqqqqq qq qqqqqq, qqqq qqqqq qq qq qqqqqq qq qqqq qqqq qqqqqqqqq.  qq qq qqq qqqqqqqqq qqqq qqq qq qqqqqq qqqqqqqq qq qqqqqqqq qq qqq qqq qqqqqqqq qqqqqqqqq.  \n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3473/4262255583.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  print(essays[0])\n"]}],"source":["print(essays[0])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T01:33:55.216224Z","iopub.status.busy":"2023-12-13T01:33:55.215721Z","iopub.status.idle":"2023-12-13T01:33:55.230736Z","shell.execute_reply":"2023-12-13T01:33:55.229426Z","shell.execute_reply.started":"2023-12-13T01:33:55.216187Z"},"trusted":true},"outputs":[],"source":["def features(df):\n","    \n","    # Create a DataFrame to store the features with a single column of IDs\n","    features = pd.DataFrame({'id': df['id'].unique()})\n","    \n","    df = constructor(df)\n","\n","\n","    # Calculate IKI for all events\n","    df['iki'] = df['down_time'].diff().fillna(0)\n","\n","    # Initialize columns for intra-word IKI and inter-word IKI with NaN\n","    df['intra_word_iki'] = np.nan\n","    df['inter_word_iki'] = np.nan\n","\n","    # Identify the start and end of words\n","    word_starts = df['word_start'] == 1\n","    word_ends = df['word_end'] == 1\n","\n","    # Compute intra-word and inter-word IKI\n","    df.loc[word_starts, 'inter_word_iki'] = df.loc[word_starts, 'iki']\n","    df.loc[~word_starts & ~word_ends, 'intra_word_iki'] = df.loc[~word_starts & ~word_ends, 'iki']\n","    \n","    # IKI FEATURES\n","\n","    # Computing median, standard deviation, and maximum IKI, intra-word IKI, and inter-word IKI\n","\n","    agg_functions = ['median', 'std', 'max']\n","    iki_basics = df.groupby('id')['iki'].agg(agg_functions).reset_index()\n","    intra_word_iki_basics = df.groupby('id')['intra_word_iki'].agg(agg_functions).reset_index()\n","    inter_word_iki_basics = df.groupby('id')['inter_word_iki'].agg(agg_functions).reset_index()\n","\n","    # Renaming the columns\n","    iki_basics.columns = ['id'] + [f'iki_{f}' for f in agg_functions]\n","    intra_word_iki_basics.columns = ['id'] + [f'intra_word_iki_{f}' for f in agg_functions]\n","    inter_word_iki_basics.columns = ['id'] + [f'inter_word_iki_{f}' for f in agg_functions]\n","\n","    # Computing number of IKIs within length intervals\n","    \n","    # Define the length intervals\n","    #intervals = [0.5, 1, 1.5, 2, 2.5, 3, np.inf]\n","    \n","\n","    # Merging IKI features\n","    features = features.merge(iki_basics, on='id')\n","    features = features.merge(intra_word_iki_basics, on='id')\n","    features = features.merge(inter_word_iki_basics, on='id')\n","\n","    # Calculate the word count for each user\n","    word_count = df.groupby('id')['word_start'].sum().reset_index()\n","    features = features.merge(word_count, on='id')\n","    \n","    # Calculate the words per minute for each user by dividing the word count by the timestamp of the last event\n","    features['wpm'] = features['word_start'] / (df.groupby('id')['up_time'].max().reset_index()['up_time']/1000/60)\n","\n","    # REVISION FEATURES\n","    \n","    return features"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"bbf8fbd2-3b3b-454d-8e21-5b9ca21d07ea","_uuid":"34de54f6-2160-472a-b088-7516a4e164fd","collapsed":false,"execution":{"iopub.execute_input":"2023-12-13T01:34:09.585634Z","iopub.status.busy":"2023-12-13T01:34:09.584784Z","iopub.status.idle":"2023-12-13T01:34:21.382967Z","shell.execute_reply":"2023-12-13T01:34:21.381919Z","shell.execute_reply.started":"2023-12-13T01:34:09.585598Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["features_train = features(df_train)\n","features_test = features(df_test)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T01:34:21.386233Z","iopub.status.busy":"2023-12-13T01:34:21.385014Z","iopub.status.idle":"2023-12-13T01:34:21.402206Z","shell.execute_reply":"2023-12-13T01:34:21.400619Z","shell.execute_reply.started":"2023-12-13T01:34:21.386193Z"},"trusted":true},"outputs":[],"source":["# Merging training features with training scores\n","df_train_merged = features_train.merge(df_train_scores, on='id')\n","\n","# Splitting the merged data into features and target variable\n","X_train = df_train_merged.drop(['id', 'score'], axis=1)  # Dropping 'id' as it's not a feature\n","y_train = df_train_merged['score']\n","\n","X_test = features_test.drop('id', axis=1)  # Dropping 'id' as it's not a feature"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n","Best parameters: {'l2_regularization': 0.7300393165618185, 'learning_rate': 0.01954322553832976, 'max_depth': 4, 'max_iter': 243, 'max_leaf_nodes': 49, 'min_samples_leaf': 1}\n"]}],"source":["# Define the parameter distribution\n","param_dist = {\n","    'learning_rate': uniform(0.01, 0.2),\n","    'max_depth': randint(3, 10),\n","    'min_samples_leaf': randint(1, 4),\n","    'max_leaf_nodes': randint(31, 51),\n","    'max_iter': randint(100, 300),\n","    'l2_regularization': uniform(0, 1)\n","}\n","\n","# Initialize the model\n","model = HistGradientBoostingRegressor()\n","\n","# Initialize the random search model\n","random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=100, cv=5, scoring='neg_mean_squared_error', verbose=1, random_state=42)\n","\n","# Fit the random search model\n","random_search.fit(X_train, y_train)\n","\n","# Best parameters\n","print(\"Best parameters:\", random_search.best_params_)"]},{"cell_type":"markdown","metadata":{},"source":["### K-Fold CV, Model Training and Preliminary Evaluation"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Mean Squared Error across folds (training): 0.5756465480887429\n","Median Mean Squared Error across folds (training): 0.5917178806800643\n"]}],"source":["# Doing K-fold cross-validation\n","kf = KFold(n_splits=10, shuffle=True, random_state=42)\n","\n","mse_scores = []\n","\n","# Loop over each fold\n","for train_index, val_index in kf.split(X_train):\n","    # Split the data into training and validation sets for this fold\n","    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n","    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n","\n","\n","    # Create and train the model\n","    model = HistGradientBoostingRegressor(**random_search.best_params_)\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Make predictions on the validation set and calculate MSE\n","    y_val_pred = model.predict(X_val_fold)\n","    mse = mean_squared_error(y_val_fold, y_val_pred)\n","    mse_scores.append(mse)\n","\n","# Calculate the average MSE across all folds\n","average_mse = np.mean(mse_scores)\n","median_mse = np.median(mse_scores)\n","print(\"Average Mean Squared Error across folds (training):\", average_mse)\n","print(\"Median Mean Squared Error across folds (training):\", median_mse)"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"091015ef-1fa1-46bd-b5b9-74a7a67fd803","_uuid":"c5609260-732d-4ed7-803b-ec244bf181d4","collapsed":false,"execution":{"iopub.execute_input":"2023-12-13T01:34:39.157852Z","iopub.status.busy":"2023-12-13T01:34:39.157275Z","iopub.status.idle":"2023-12-13T01:34:39.207492Z","shell.execute_reply":"2023-12-13T01:34:39.206449Z","shell.execute_reply.started":"2023-12-13T01:34:39.157817Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Predict\n","y_test_pred = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Test Prediction (for Kaggle)"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"9146d08c-da5c-4d95-be9f-ade184f62b58","_uuid":"6a5fe978-e11f-4201-a1ba-6c28e93beb1c","collapsed":false,"execution":{"iopub.execute_input":"2023-12-13T01:34:39.215962Z","iopub.status.busy":"2023-12-13T01:34:39.213159Z","iopub.status.idle":"2023-12-13T01:34:39.226734Z","shell.execute_reply":"2023-12-13T01:34:39.225823Z","shell.execute_reply.started":"2023-12-13T01:34:39.215916Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame({\n","    'id': features_test['id'],\n","    'score': y_test_pred\n","})\n","\n","# Export the submission DataFrame to a CSV file\n","submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T01:34:39.230024Z","iopub.status.busy":"2023-12-13T01:34:39.229385Z","iopub.status.idle":"2023-12-13T01:34:39.238869Z","shell.execute_reply":"2023-12-13T01:34:39.237731Z","shell.execute_reply.started":"2023-12-13T01:34:39.229989Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[3.03786344 1.80315817 2.66569514]\n"]}],"source":["print(y_test_pred)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"}],"dockerImageVersionId":30615,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
